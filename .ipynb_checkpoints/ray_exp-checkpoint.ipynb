{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Space'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3ec475c124eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mppo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPPOTrainer\u001b[0m\u001b[0;31m# , DEFAULT_CONFIG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpretty_print\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pets2/lib/python3.6/site-packages/ray/rllib/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_agent_env\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultiAgentEnv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_env\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVectorEnv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout_worker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRolloutWorker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_batch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSampleBatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pets2/lib/python3.6/site-packages/ray/rllib/evaluation/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultiAgentEpisode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout_worker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRolloutWorker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m from ray.rllib.evaluation.sample_batch_builder import (\n\u001b[1;32m      4\u001b[0m     SampleBatchBuilder, MultiAgentSampleBatchBuilder)\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSyncSampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAsyncSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pets2/lib/python3.6/site-packages/ray/rllib/evaluation/episode.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_env\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_DUMMY_AGENT_ID\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDeveloperAPI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspace_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mflatten_to_single_ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pets2/lib/python3.6/site-packages/ray/rllib/policy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch_policy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTorchPolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_policy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTFPolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch_policy_template\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_torch_policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_policy_template\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_tf_policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pets2/lib/python3.6/site-packages/ray/rllib/policy/policy.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_requirement\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mViewRequirement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDeveloperAPI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexploration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexploration\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExploration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtry_import_torch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfrom_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pets2/lib/python3.6/site-packages/ray/rllib/utils/exploration/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexploration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuriosity\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCuriosity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexploration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexploration\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExploration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexploration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon_greedy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEpsilonGreedy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexploration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgaussian_noise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGaussianNoise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexploration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mornstein_uhlenbeck_noise\u001b[0m \u001b[0;32mimport\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pets2/lib/python3.6/site-packages/ray/rllib/utils/exploration/curiosity.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspaces\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDiscrete\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultiDiscrete\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSpace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_dist\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mActionDistribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Space'"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import gym\n",
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer, DEFAULT_CONFIG\n",
    "from ray.tune.logger import pretty_print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    " from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import gym\n",
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer, DEFAULT_CONFIG\n",
    "from ray.tune.logger import pretty_print\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-26 21:11:44,456\tERROR worker.py:660 -- Calling ray.init() again after it has already been called.\n"
     ]
    }
   ],
   "source": [
    "# Start up Ray. This must be done before we instantiate any RL agents.\n",
    "ray.init(num_cpus=1, ignore_reinit_error=True, log_to_driver=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-26 21:13:26,255\tWARNING worker.py:1091 -- WARNING: 9 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-11-26 21:13:26,297\tWARNING worker.py:1091 -- WARNING: 10 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "/data/ShenShuo/miniconda3/envs/pets2/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/data/ShenShuo/miniconda3/envs/pets2/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/data/ShenShuo/miniconda3/envs/pets2/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "2020-11-26 21:13:36,667\tINFO trainable.py:255 -- Trainable.setup took 11.014 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2020-11-26 21:13:36,670\tWARNING util.py:40 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "config = DEFAULT_CONFIG.copy()\n",
    "config['num_workers'] = 4\n",
    "config['num_sgd_iter'] = 10\n",
    "config['sgd_minibatch_size'] = 521\n",
    "config['model']['fcnet_hiddens'] = [64, 64]\n",
    "config['num_cpus_per_worker'] = 0  # This avoids running out of resources in the notebook environment when this cell is re-executed\n",
    "\n",
    "agent = PPOTrainer(config, 'CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_metrics: {}\n",
      "date: 2020-11-26_21-13-39\n",
      "done: false\n",
      "episode_len_mean: 21.193548387096776\n",
      "episode_reward_max: 58.0\n",
      "episode_reward_mean: 21.193548387096776\n",
      "episode_reward_min: 8.0\n",
      "episodes_this_iter: 186\n",
      "episodes_total: 186\n",
      "experiment_id: 7493f658af8b447c956cf9373146e196\n",
      "hostname: gpu-53\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 0.20000000298023224\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6905393600463867\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.0024201315827667713\n",
      "      model: {}\n",
      "      policy_loss: -0.005955063737928867\n",
      "      total_loss: 203.9124298095703\n",
      "      vf_explained_var: 9.518010483589023e-05\n",
      "      vf_loss: 203.9179229736328\n",
      "  num_steps_sampled: 4000\n",
      "  num_steps_trained: 4000\n",
      "iterations_since_restore: 1\n",
      "node_ip: 172.31.246.53\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 18.46\n",
      "  ram_util_percent: 14.12\n",
      "pid: 22008\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0816400272932044\n",
      "  mean_env_wait_ms: 0.07699236195172436\n",
      "  mean_inference_ms: 1.0030760965915544\n",
      "  mean_raw_obs_processing_ms: 0.11314200244496374\n",
      "time_since_restore: 3.3089962005615234\n",
      "time_this_iter_s: 3.3089962005615234\n",
      "time_total_s: 3.3089962005615234\n",
      "timers:\n",
      "  learn_throughput: 2639.044\n",
      "  learn_time_ms: 1515.701\n",
      "  load_throughput: 34059.604\n",
      "  load_time_ms: 117.441\n",
      "  sample_throughput: 2850.862\n",
      "  sample_time_ms: 1403.084\n",
      "  update_time_ms: 3.554\n",
      "timestamp: 1606396419\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 4000\n",
      "training_iteration: 1\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-11-26_21-13-42\n",
      "done: false\n",
      "episode_len_mean: 22.732954545454547\n",
      "episode_reward_max: 76.0\n",
      "episode_reward_mean: 22.732954545454547\n",
      "episode_reward_min: 9.0\n",
      "episodes_this_iter: 176\n",
      "episodes_total: 362\n",
      "experiment_id: 7493f658af8b447c956cf9373146e196\n",
      "hostname: gpu-53\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 0.10000000149011612\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6793023347854614\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.004422846250236034\n",
      "      model: {}\n",
      "      policy_loss: -0.015973184257745743\n",
      "      total_loss: 235.43479919433594\n",
      "      vf_explained_var: -8.574553794460371e-05\n",
      "      vf_loss: 235.45033264160156\n",
      "  num_steps_sampled: 8000\n",
      "  num_steps_trained: 8000\n",
      "iterations_since_restore: 2\n",
      "node_ip: 172.31.246.53\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 20.125\n",
      "  ram_util_percent: 14.0\n",
      "pid: 22008\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08144897261242577\n",
      "  mean_env_wait_ms: 0.07802823033869433\n",
      "  mean_inference_ms: 0.9709964400337324\n",
      "  mean_raw_obs_processing_ms: 0.11385687788571251\n",
      "time_since_restore: 5.673088788986206\n",
      "time_this_iter_s: 2.3640925884246826\n",
      "time_total_s: 5.673088788986206\n",
      "timers:\n",
      "  learn_throughput: 3154.954\n",
      "  learn_time_ms: 1267.847\n",
      "  load_throughput: 66974.379\n",
      "  load_time_ms: 59.724\n",
      "  sample_throughput: 2954.51\n",
      "  sample_time_ms: 1353.863\n",
      "  update_time_ms: 3.753\n",
      "timestamp: 1606396422\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 8000\n",
      "training_iteration: 2\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-11-26_21-13-44\n",
      "done: false\n",
      "episode_len_mean: 32.314516129032256\n",
      "episode_reward_max: 107.0\n",
      "episode_reward_mean: 32.314516129032256\n",
      "episode_reward_min: 9.0\n",
      "episodes_this_iter: 124\n",
      "episodes_total: 486\n",
      "experiment_id: 7493f658af8b447c956cf9373146e196\n",
      "hostname: gpu-53\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 0.05000000074505806\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6623197793960571\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.0046443394385278225\n",
      "      model: {}\n",
      "      policy_loss: -0.011120949871838093\n",
      "      total_loss: 438.5076599121094\n",
      "      vf_explained_var: 0.0034644773695617914\n",
      "      vf_loss: 438.51861572265625\n",
      "  num_steps_sampled: 12000\n",
      "  num_steps_trained: 12000\n",
      "iterations_since_restore: 3\n",
      "node_ip: 172.31.246.53\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 21.0\n",
      "  ram_util_percent: 14.0\n",
      "pid: 22008\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08115510449726909\n",
      "  mean_env_wait_ms: 0.07785613659515299\n",
      "  mean_inference_ms: 0.9623690118992744\n",
      "  mean_raw_obs_processing_ms: 0.11087647102621583\n",
      "time_since_restore: 8.1031653881073\n",
      "time_this_iter_s: 2.4300765991210938\n",
      "time_total_s: 8.1031653881073\n",
      "timers:\n",
      "  learn_throughput: 3331.176\n",
      "  learn_time_ms: 1200.777\n",
      "  load_throughput: 98759.606\n",
      "  load_time_ms: 40.502\n",
      "  sample_throughput: 2969.77\n",
      "  sample_time_ms: 1346.906\n",
      "  update_time_ms: 3.61\n",
      "timestamp: 1606396424\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 12000\n",
      "training_iteration: 3\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-11-26_21-13-47\n",
      "done: false\n",
      "episode_len_mean: 36.71559633027523\n",
      "episode_reward_max: 138.0\n",
      "episode_reward_mean: 36.71559633027523\n",
      "episode_reward_min: 11.0\n",
      "episodes_this_iter: 109\n",
      "episodes_total: 595\n",
      "experiment_id: 7493f658af8b447c956cf9373146e196\n",
      "hostname: gpu-53\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 0.02500000037252903\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6414509415626526\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.0037498956080526114\n",
      "      model: {}\n",
      "      policy_loss: -0.009251202456653118\n",
      "      total_loss: 565.3070068359375\n",
      "      vf_explained_var: -0.0007153834449127316\n",
      "      vf_loss: 565.316162109375\n",
      "  num_steps_sampled: 16000\n",
      "  num_steps_trained: 16000\n",
      "iterations_since_restore: 4\n",
      "node_ip: 172.31.246.53\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 19.325\n",
      "  ram_util_percent: 14.0\n",
      "pid: 22008\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08025813575995966\n",
      "  mean_env_wait_ms: 0.07742808362520404\n",
      "  mean_inference_ms: 0.9468717542205944\n",
      "  mean_raw_obs_processing_ms: 0.10921497038427919\n",
      "time_since_restore: 10.48327922821045\n",
      "time_this_iter_s: 2.3801138401031494\n",
      "time_total_s: 10.48327922821045\n",
      "timers:\n",
      "  learn_throughput: 3437.838\n",
      "  learn_time_ms: 1163.522\n",
      "  load_throughput: 129323.356\n",
      "  load_time_ms: 30.93\n",
      "  sample_throughput: 3001.681\n",
      "  sample_time_ms: 1332.587\n",
      "  update_time_ms: 3.729\n",
      "timestamp: 1606396427\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 16000\n",
      "training_iteration: 4\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-11-26_21-13-49\n",
      "done: false\n",
      "episode_len_mean: 45.71\n",
      "episode_reward_max: 159.0\n",
      "episode_reward_mean: 45.71\n",
      "episode_reward_min: 11.0\n",
      "episodes_this_iter: 83\n",
      "episodes_total: 678\n",
      "experiment_id: 7493f658af8b447c956cf9373146e196\n",
      "hostname: gpu-53\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 0.012500000186264515\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6261349320411682\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.003036095993593335\n",
      "      model: {}\n",
      "      policy_loss: -0.01032688282430172\n",
      "      total_loss: 818.0867309570312\n",
      "      vf_explained_var: 0.00011666332284221426\n",
      "      vf_loss: 818.0970458984375\n",
      "  num_steps_sampled: 20000\n",
      "  num_steps_trained: 20000\n",
      "iterations_since_restore: 5\n",
      "node_ip: 172.31.246.53\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 21.400000000000002\n",
      "  ram_util_percent: 14.0\n",
      "pid: 22008\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08120013558674477\n",
      "  mean_env_wait_ms: 0.07803540456354942\n",
      "  mean_inference_ms: 0.9506659084722408\n",
      "  mean_raw_obs_processing_ms: 0.1082339107411058\n",
      "time_since_restore: 12.970354318618774\n",
      "time_this_iter_s: 2.487075090408325\n",
      "time_total_s: 12.970354318618774\n",
      "timers:\n",
      "  learn_throughput: 3509.943\n",
      "  learn_time_ms: 1139.62\n",
      "  load_throughput: 158993.795\n",
      "  load_time_ms: 25.158\n",
      "  sample_throughput: 2969.063\n",
      "  sample_time_ms: 1347.226\n",
      "  update_time_ms: 3.708\n",
      "timestamp: 1606396429\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 20000\n",
      "training_iteration: 5\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-11-26_21-13-52\n",
      "done: false\n",
      "episode_len_mean: 49.35\n",
      "episode_reward_max: 159.0\n",
      "episode_reward_mean: 49.35\n",
      "episode_reward_min: 14.0\n",
      "episodes_this_iter: 77\n",
      "episodes_total: 755\n",
      "experiment_id: 7493f658af8b447c956cf9373146e196\n",
      "hostname: gpu-53\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 0.0062500000931322575\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6124538779258728\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.001866543898358941\n",
      "      model: {}\n",
      "      policy_loss: -0.0014927041484043002\n",
      "      total_loss: 832.0755004882812\n",
      "      vf_explained_var: -0.00763092702254653\n",
      "      vf_loss: 832.0769653320312\n",
      "  num_steps_sampled: 24000\n",
      "  num_steps_trained: 24000\n",
      "iterations_since_restore: 6\n",
      "node_ip: 172.31.246.53\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 19.724999999999998\n",
      "  ram_util_percent: 14.0\n",
      "pid: 22008\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08202210508502676\n",
      "  mean_env_wait_ms: 0.0789752923024111\n",
      "  mean_inference_ms: 0.9607867622748734\n",
      "  mean_raw_obs_processing_ms: 0.10837188685299318\n",
      "time_since_restore: 15.41814136505127\n",
      "time_this_iter_s: 2.447787046432495\n",
      "time_total_s: 15.41814136505127\n",
      "timers:\n",
      "  learn_throughput: 3568.974\n",
      "  learn_time_ms: 1120.77\n",
      "  load_throughput: 187246.063\n",
      "  load_time_ms: 21.362\n",
      "  sample_throughput: 2956.362\n",
      "  sample_time_ms: 1353.014\n",
      "  update_time_ms: 3.73\n",
      "timestamp: 1606396432\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 24000\n",
      "training_iteration: 6\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_metrics: {}\n",
      "date: 2020-11-26_21-13-54\n",
      "done: false\n",
      "episode_len_mean: 58.52\n",
      "episode_reward_max: 183.0\n",
      "episode_reward_mean: 58.52\n",
      "episode_reward_min: 15.0\n",
      "episodes_this_iter: 67\n",
      "episodes_total: 822\n",
      "experiment_id: 7493f658af8b447c956cf9373146e196\n",
      "hostname: gpu-53\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 0.0031250000465661287\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5976220369338989\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.0026260479353368282\n",
      "      model: {}\n",
      "      policy_loss: -0.003128698328509927\n",
      "      total_loss: 912.3282470703125\n",
      "      vf_explained_var: -0.004735180176794529\n",
      "      vf_loss: 912.3311767578125\n",
      "  num_steps_sampled: 28000\n",
      "  num_steps_trained: 28000\n",
      "iterations_since_restore: 7\n",
      "node_ip: 172.31.246.53\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 21.73333333333333\n",
      "  ram_util_percent: 14.0\n",
      "pid: 22008\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08244866922625203\n",
      "  mean_env_wait_ms: 0.0790324802629635\n",
      "  mean_inference_ms: 0.9603222773422374\n",
      "  mean_raw_obs_processing_ms: 0.10744123233954932\n",
      "time_since_restore: 17.862040042877197\n",
      "time_this_iter_s: 2.4438986778259277\n",
      "time_total_s: 17.862040042877197\n",
      "timers:\n",
      "  learn_throughput: 3600.017\n",
      "  learn_time_ms: 1111.106\n",
      "  load_throughput: 214939.772\n",
      "  load_time_ms: 18.61\n",
      "  sample_throughput: 2955.468\n",
      "  sample_time_ms: 1353.424\n",
      "  update_time_ms: 3.631\n",
      "timestamp: 1606396434\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 28000\n",
      "training_iteration: 7\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-11-26_21-13-57\n",
      "done: false\n",
      "episode_len_mean: 75.41\n",
      "episode_reward_max: 183.0\n",
      "episode_reward_mean: 75.41\n",
      "episode_reward_min: 17.0\n",
      "episodes_this_iter: 43\n",
      "episodes_total: 865\n",
      "experiment_id: 7493f658af8b447c956cf9373146e196\n",
      "hostname: gpu-53\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 0.0015625000232830644\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6014923453330994\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.0031622592359781265\n",
      "      model: {}\n",
      "      policy_loss: -0.0063764117658138275\n",
      "      total_loss: 1494.716552734375\n",
      "      vf_explained_var: 0.012990610674023628\n",
      "      vf_loss: 1494.7227783203125\n",
      "  num_steps_sampled: 32000\n",
      "  num_steps_trained: 32000\n",
      "iterations_since_restore: 8\n",
      "node_ip: 172.31.246.53\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 19.375\n",
      "  ram_util_percent: 14.0\n",
      "pid: 22008\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08259070196975542\n",
      "  mean_env_wait_ms: 0.07962161176435481\n",
      "  mean_inference_ms: 0.9687893055006219\n",
      "  mean_raw_obs_processing_ms: 0.10747723137717462\n",
      "time_since_restore: 20.32242774963379\n",
      "time_this_iter_s: 2.460387706756592\n",
      "time_total_s: 20.32242774963379\n",
      "timers:\n",
      "  learn_throughput: 3622.928\n",
      "  learn_time_ms: 1104.079\n",
      "  load_throughput: 239960.324\n",
      "  load_time_ms: 16.669\n",
      "  sample_throughput: 2952.002\n",
      "  sample_time_ms: 1355.012\n",
      "  update_time_ms: 3.585\n",
      "timestamp: 1606396437\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 32000\n",
      "training_iteration: 8\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-11-26_21-13-59\n",
      "done: false\n",
      "episode_len_mean: 90.19\n",
      "episode_reward_max: 200.0\n",
      "episode_reward_mean: 90.19\n",
      "episode_reward_min: 19.0\n",
      "episodes_this_iter: 39\n",
      "episodes_total: 904\n",
      "experiment_id: 7493f658af8b447c956cf9373146e196\n",
      "hostname: gpu-53\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 0.0007812500116415322\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5931504964828491\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.0013908020919188857\n",
      "      model: {}\n",
      "      policy_loss: -0.006660066079348326\n",
      "      total_loss: 1651.464599609375\n",
      "      vf_explained_var: 0.007118523120880127\n",
      "      vf_loss: 1651.4710693359375\n",
      "  num_steps_sampled: 36000\n",
      "  num_steps_trained: 36000\n",
      "iterations_since_restore: 9\n",
      "node_ip: 172.31.246.53\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 21.3\n",
      "  ram_util_percent: 14.0\n",
      "pid: 22008\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08335682062225132\n",
      "  mean_env_wait_ms: 0.08020209662969251\n",
      "  mean_inference_ms: 0.9720136898326517\n",
      "  mean_raw_obs_processing_ms: 0.10746193789840613\n",
      "time_since_restore: 22.875786304473877\n",
      "time_this_iter_s: 2.553358554840088\n",
      "time_total_s: 22.875786304473877\n",
      "timers:\n",
      "  learn_throughput: 3649.533\n",
      "  learn_time_ms: 1096.031\n",
      "  load_throughput: 264397.808\n",
      "  load_time_ms: 15.129\n",
      "  sample_throughput: 2921.512\n",
      "  sample_time_ms: 1369.154\n",
      "  update_time_ms: 3.558\n",
      "timestamp: 1606396439\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 36000\n",
      "training_iteration: 9\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-11-26_21-14-02\n",
      "done: false\n",
      "episode_len_mean: 101.95\n",
      "episode_reward_max: 200.0\n",
      "episode_reward_mean: 101.95\n",
      "episode_reward_min: 21.0\n",
      "episodes_this_iter: 36\n",
      "episodes_total: 940\n",
      "experiment_id: 7493f658af8b447c956cf9373146e196\n",
      "hostname: gpu-53\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 0.0003906250058207661\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5842944979667664\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.0022014847490936518\n",
      "      model: {}\n",
      "      policy_loss: 0.0033430333714932203\n",
      "      total_loss: 1515.521240234375\n",
      "      vf_explained_var: -0.020355377346277237\n",
      "      vf_loss: 1515.517822265625\n",
      "  num_steps_sampled: 40000\n",
      "  num_steps_trained: 40000\n",
      "iterations_since_restore: 10\n",
      "node_ip: 172.31.246.53\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 19.775\n",
      "  ram_util_percent: 14.0\n",
      "pid: 22008\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08360484732211702\n",
      "  mean_env_wait_ms: 0.08045376043255954\n",
      "  mean_inference_ms: 0.977249844399516\n",
      "  mean_raw_obs_processing_ms: 0.10699448755116667\n",
      "time_since_restore: 25.329248905181885\n",
      "time_this_iter_s: 2.453462600708008\n",
      "time_total_s: 25.329248905181885\n",
      "timers:\n",
      "  learn_throughput: 3663.599\n",
      "  learn_time_ms: 1091.822\n",
      "  load_throughput: 289690.888\n",
      "  load_time_ms: 13.808\n",
      "  sample_throughput: 2920.428\n",
      "  sample_time_ms: 1369.662\n",
      "  update_time_ms: 3.601\n",
      "timestamp: 1606396442\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 40000\n",
      "training_iteration: 10\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-11-26_21-14-04\n",
      "done: false\n",
      "episode_len_mean: 113.97\n",
      "episode_reward_max: 200.0\n",
      "episode_reward_mean: 113.97\n",
      "episode_reward_min: 16.0\n",
      "episodes_this_iter: 29\n",
      "episodes_total: 969\n",
      "experiment_id: 7493f658af8b447c956cf9373146e196\n",
      "hostname: gpu-53\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 0.00019531250291038305\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.590885579586029\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.0012019125279039145\n",
      "      model: {}\n",
      "      policy_loss: -0.006026549730449915\n",
      "      total_loss: 2093.656005859375\n",
      "      vf_explained_var: 0.04769640415906906\n",
      "      vf_loss: 2093.661865234375\n",
      "  num_steps_sampled: 44000\n",
      "  num_steps_trained: 44000\n",
      "iterations_since_restore: 11\n",
      "node_ip: 172.31.246.53\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 19.325000000000003\n",
      "  ram_util_percent: 14.0\n",
      "pid: 22008\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08373697590010706\n",
      "  mean_env_wait_ms: 0.08089996144970767\n",
      "  mean_inference_ms: 0.9822882819588359\n",
      "  mean_raw_obs_processing_ms: 0.10694307347251955\n",
      "time_since_restore: 27.80670142173767\n",
      "time_this_iter_s: 2.477452516555786\n",
      "time_total_s: 27.80670142173767\n",
      "timers:\n",
      "  learn_throughput: 3825.683\n",
      "  learn_time_ms: 1045.565\n",
      "  load_throughput: 1722365.309\n",
      "  load_time_ms: 2.322\n",
      "  sample_throughput: 2924.423\n",
      "  sample_time_ms: 1367.791\n",
      "  update_time_ms: 3.68\n",
      "timestamp: 1606396444\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 44000\n",
      "training_iteration: 11\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-11-26_21-14-07\n",
      "done: false\n",
      "episode_len_mean: 127.02\n",
      "episode_reward_max: 200.0\n",
      "episode_reward_mean: 127.02\n",
      "episode_reward_min: 16.0\n",
      "episodes_this_iter: 29\n",
      "episodes_total: 998\n",
      "experiment_id: 7493f658af8b447c956cf9373146e196\n",
      "hostname: gpu-53\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 9.765625145519152e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5795878767967224\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.00043708572047762573\n",
      "      model: {}\n",
      "      policy_loss: -0.004023328889161348\n",
      "      total_loss: 1741.8697509765625\n",
      "      vf_explained_var: 0.011910557746887207\n",
      "      vf_loss: 1741.8739013671875\n",
      "  num_steps_sampled: 48000\n",
      "  num_steps_trained: 48000\n",
      "iterations_since_restore: 12\n",
      "node_ip: 172.31.246.53\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 21.066666666666666\n",
      "  ram_util_percent: 14.0\n",
      "pid: 22008\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08391751796123441\n",
      "  mean_env_wait_ms: 0.08099979025106614\n",
      "  mean_inference_ms: 0.9801072313174007\n",
      "  mean_raw_obs_processing_ms: 0.10635269149791361\n",
      "time_since_restore: 30.306230783462524\n",
      "time_this_iter_s: 2.4995293617248535\n",
      "time_total_s: 30.306230783462524\n",
      "timers:\n",
      "  learn_throughput: 3814.683\n",
      "  learn_time_ms: 1048.58\n",
      "  load_throughput: 1753031.848\n",
      "  load_time_ms: 2.282\n",
      "  sample_throughput: 2902.171\n",
      "  sample_time_ms: 1378.278\n",
      "  update_time_ms: 3.831\n",
      "timestamp: 1606396447\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 48000\n",
      "training_iteration: 12\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_metrics: {}\n",
      "date: 2020-11-26_21-14-09\n",
      "done: false\n",
      "episode_len_mean: 136.02\n",
      "episode_reward_max: 200.0\n",
      "episode_reward_mean: 136.02\n",
      "episode_reward_min: 16.0\n",
      "episodes_this_iter: 27\n",
      "episodes_total: 1025\n",
      "experiment_id: 7493f658af8b447c956cf9373146e196\n",
      "hostname: gpu-53\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 4.882812572759576e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5737748146057129\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.00046244257828220725\n",
      "      model: {}\n",
      "      policy_loss: 0.00048213344416581094\n",
      "      total_loss: 1793.5740966796875\n",
      "      vf_explained_var: 0.03097328171133995\n",
      "      vf_loss: 1793.573486328125\n",
      "  num_steps_sampled: 52000\n",
      "  num_steps_trained: 52000\n",
      "iterations_since_restore: 13\n",
      "node_ip: 172.31.246.53\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 19.375\n",
      "  ram_util_percent: 14.0\n",
      "pid: 22008\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08397009196287207\n",
      "  mean_env_wait_ms: 0.08089607977614151\n",
      "  mean_inference_ms: 0.9788592644010277\n",
      "  mean_raw_obs_processing_ms: 0.10559506673602964\n",
      "time_since_restore: 32.76800560951233\n",
      "time_this_iter_s: 2.4617748260498047\n",
      "time_total_s: 32.76800560951233\n",
      "timers:\n",
      "  learn_throughput: 3830.8\n",
      "  learn_time_ms: 1044.168\n",
      "  load_throughput: 1703305.245\n",
      "  load_time_ms: 2.348\n",
      "  sample_throughput: 2886.493\n",
      "  sample_time_ms: 1385.765\n",
      "  update_time_ms: 3.829\n",
      "timestamp: 1606396449\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 52000\n",
      "training_iteration: 13\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-11-26_21-14-12\n",
      "done: false\n",
      "episode_len_mean: 145.44\n",
      "episode_reward_max: 200.0\n",
      "episode_reward_mean: 145.44\n",
      "episode_reward_min: 16.0\n",
      "episodes_this_iter: 26\n",
      "episodes_total: 1051\n",
      "experiment_id: 7493f658af8b447c956cf9373146e196\n",
      "hostname: gpu-53\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 2.441406286379788e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5835129022598267\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.00047370183165185153\n",
      "      model: {}\n",
      "      policy_loss: 0.002179371891543269\n",
      "      total_loss: 1957.8564453125\n",
      "      vf_explained_var: -0.0013845136854797602\n",
      "      vf_loss: 1957.8543701171875\n",
      "  num_steps_sampled: 56000\n",
      "  num_steps_trained: 56000\n",
      "iterations_since_restore: 14\n",
      "node_ip: 172.31.246.53\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 20.666666666666668\n",
      "  ram_util_percent: 14.0\n",
      "pid: 22008\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08400361664367395\n",
      "  mean_env_wait_ms: 0.08098184594228446\n",
      "  mean_inference_ms: 0.9811157305997835\n",
      "  mean_raw_obs_processing_ms: 0.10523017744034054\n",
      "time_since_restore: 35.20807218551636\n",
      "time_this_iter_s: 2.4400665760040283\n",
      "time_total_s: 35.20807218551636\n",
      "timers:\n",
      "  learn_throughput: 3840.95\n",
      "  learn_time_ms: 1041.409\n",
      "  load_throughput: 1687424.29\n",
      "  load_time_ms: 2.37\n",
      "  sample_throughput: 2867.356\n",
      "  sample_time_ms: 1395.013\n",
      "  update_time_ms: 3.738\n",
      "timestamp: 1606396452\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 56000\n",
      "training_iteration: 14\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-11-26_21-14-14\n",
      "done: false\n",
      "episode_len_mean: 149.93\n",
      "episode_reward_max: 200.0\n",
      "episode_reward_mean: 149.93\n",
      "episode_reward_min: 30.0\n",
      "episodes_this_iter: 26\n",
      "episodes_total: 1077\n",
      "experiment_id: 7493f658af8b447c956cf9373146e196\n",
      "hostname: gpu-53\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 1.220703143189894e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5677341818809509\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.001555421156808734\n",
      "      model: {}\n",
      "      policy_loss: 0.0030452455393970013\n",
      "      total_loss: 1780.3525390625\n",
      "      vf_explained_var: 0.004125109873712063\n",
      "      vf_loss: 1780.349609375\n",
      "  num_steps_sampled: 60000\n",
      "  num_steps_trained: 60000\n",
      "iterations_since_restore: 15\n",
      "node_ip: 172.31.246.53\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 19.6\n",
      "  ram_util_percent: 14.0\n",
      "pid: 22008\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08402444441380276\n",
      "  mean_env_wait_ms: 0.08098747582433685\n",
      "  mean_inference_ms: 0.9812919838863121\n",
      "  mean_raw_obs_processing_ms: 0.10481327087599361\n",
      "time_since_restore: 37.62865877151489\n",
      "time_this_iter_s: 2.420586585998535\n",
      "time_total_s: 37.62865877151489\n",
      "timers:\n",
      "  learn_throughput: 3838.122\n",
      "  learn_time_ms: 1042.176\n",
      "  load_throughput: 1699251.112\n",
      "  load_time_ms: 2.354\n",
      "  sample_throughput: 2882.771\n",
      "  sample_time_ms: 1387.554\n",
      "  update_time_ms: 3.727\n",
      "timestamp: 1606396454\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 60000\n",
      "training_iteration: 15\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-11-26_21-14-17\n",
      "done: false\n",
      "episode_len_mean: 157.35\n",
      "episode_reward_max: 200.0\n",
      "episode_reward_mean: 157.35\n",
      "episode_reward_min: 30.0\n",
      "episodes_this_iter: 22\n",
      "episodes_total: 1099\n",
      "experiment_id: 7493f658af8b447c956cf9373146e196\n",
      "hostname: gpu-53\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 6.10351571594947e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5568482875823975\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.0014268632512539625\n",
      "      model: {}\n",
      "      policy_loss: -0.004794607870280743\n",
      "      total_loss: 2037.786865234375\n",
      "      vf_explained_var: 0.004979431629180908\n",
      "      vf_loss: 2037.791748046875\n",
      "  num_steps_sampled: 64000\n",
      "  num_steps_trained: 64000\n",
      "iterations_since_restore: 16\n",
      "node_ip: 172.31.246.53\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 20.666666666666668\n",
      "  ram_util_percent: 14.0\n",
      "pid: 22008\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08407431222873503\n",
      "  mean_env_wait_ms: 0.08118752291621176\n",
      "  mean_inference_ms: 0.9834532737754195\n",
      "  mean_raw_obs_processing_ms: 0.10476948296334897\n",
      "time_since_restore: 40.03538799285889\n",
      "time_this_iter_s: 2.406729221343994\n",
      "time_total_s: 40.03538799285889\n",
      "timers:\n",
      "  learn_throughput: 3840.798\n",
      "  learn_time_ms: 1041.45\n",
      "  load_throughput: 1626187.711\n",
      "  load_time_ms: 2.46\n",
      "  sample_throughput: 2890.76\n",
      "  sample_time_ms: 1383.719\n",
      "  update_time_ms: 3.879\n",
      "timestamp: 1606396457\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 64000\n",
      "training_iteration: 16\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-11-26_21-14-19\n",
      "done: false\n",
      "episode_len_mean: 163.71\n",
      "episode_reward_max: 200.0\n",
      "episode_reward_mean: 163.71\n",
      "episode_reward_min: 42.0\n",
      "episodes_this_iter: 23\n",
      "episodes_total: 1122\n",
      "experiment_id: 7493f658af8b447c956cf9373146e196\n",
      "hostname: gpu-53\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 3.051757857974735e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5512293577194214\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.003900961484760046\n",
      "      model: {}\n",
      "      policy_loss: 0.0034529000986367464\n",
      "      total_loss: 2069.21240234375\n",
      "      vf_explained_var: 0.014841624535620213\n",
      "      vf_loss: 2069.208740234375\n",
      "  num_steps_sampled: 68000\n",
      "  num_steps_trained: 68000\n",
      "iterations_since_restore: 17\n",
      "node_ip: 172.31.246.53\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 20.175000000000004\n",
      "  ram_util_percent: 14.0\n",
      "pid: 22008\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08399288619035783\n",
      "  mean_env_wait_ms: 0.08111925447854978\n",
      "  mean_inference_ms: 0.9829815275177575\n",
      "  mean_raw_obs_processing_ms: 0.10438586634343056\n",
      "time_since_restore: 42.5078501701355\n",
      "time_this_iter_s: 2.4724621772766113\n",
      "time_total_s: 42.5078501701355\n",
      "timers:\n",
      "  learn_throughput: 3844.165\n",
      "  learn_time_ms: 1040.538\n",
      "  load_throughput: 1540862.218\n",
      "  load_time_ms: 2.596\n",
      "  sample_throughput: 2883.702\n",
      "  sample_time_ms: 1387.106\n",
      "  update_time_ms: 4.161\n",
      "timestamp: 1606396459\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 68000\n",
      "training_iteration: 17\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-11-26_21-14-22\n",
      "done: false\n",
      "episode_len_mean: 168.16\n",
      "episode_reward_max: 200.0\n",
      "episode_reward_mean: 168.16\n",
      "episode_reward_min: 42.0\n",
      "episodes_this_iter: 23\n",
      "episodes_total: 1145\n",
      "experiment_id: 7493f658af8b447c956cf9373146e196\n",
      "hostname: gpu-53\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 1.5258789289873675e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5592685341835022\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.0018885763129219413\n",
      "      model: {}\n",
      "      policy_loss: -0.0013249346520751715\n",
      "      total_loss: 2273.592529296875\n",
      "      vf_explained_var: 0.024823861196637154\n",
      "      vf_loss: 2273.593994140625\n",
      "  num_steps_sampled: 72000\n",
      "  num_steps_trained: 72000\n",
      "iterations_since_restore: 18\n",
      "node_ip: 172.31.246.53\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 21.53333333333333\n",
      "  ram_util_percent: 14.0\n",
      "pid: 22008\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08393133542369906\n",
      "  mean_env_wait_ms: 0.08105041602140725\n",
      "  mean_inference_ms: 0.9818133802966084\n",
      "  mean_raw_obs_processing_ms: 0.10402387856692163\n",
      "time_since_restore: 44.94430637359619\n",
      "time_this_iter_s: 2.4364562034606934\n",
      "time_total_s: 44.94430637359619\n",
      "timers:\n",
      "  learn_throughput: 3851.782\n",
      "  learn_time_ms: 1038.48\n",
      "  load_throughput: 1588917.027\n",
      "  load_time_ms: 2.517\n",
      "  sample_throughput: 2883.272\n",
      "  sample_time_ms: 1387.313\n",
      "  update_time_ms: 4.183\n",
      "timestamp: 1606396462\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 72000\n",
      "training_iteration: 18\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_metrics: {}\n",
      "date: 2020-11-26_21-14-24\n",
      "done: false\n",
      "episode_len_mean: 173.11\n",
      "episode_reward_max: 200.0\n",
      "episode_reward_mean: 173.11\n",
      "episode_reward_min: 42.0\n",
      "episodes_this_iter: 22\n",
      "episodes_total: 1167\n",
      "experiment_id: 7493f658af8b447c956cf9373146e196\n",
      "hostname: gpu-53\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 7.629394644936838e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5630360245704651\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.0015166668454185128\n",
      "      model: {}\n",
      "      policy_loss: -0.0038979414384812117\n",
      "      total_loss: 2151.814208984375\n",
      "      vf_explained_var: 0.01709512248635292\n",
      "      vf_loss: 2151.81787109375\n",
      "  num_steps_sampled: 76000\n",
      "  num_steps_trained: 76000\n",
      "iterations_since_restore: 19\n",
      "node_ip: 172.31.246.53\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 19.4\n",
      "  ram_util_percent: 14.0\n",
      "pid: 22008\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0839535424569732\n",
      "  mean_env_wait_ms: 0.08103727270845283\n",
      "  mean_inference_ms: 0.9819994218935284\n",
      "  mean_raw_obs_processing_ms: 0.10372092969374005\n",
      "time_since_restore: 47.390143632888794\n",
      "time_this_iter_s: 2.4458372592926025\n",
      "time_total_s: 47.390143632888794\n",
      "timers:\n",
      "  learn_throughput: 3852.33\n",
      "  learn_time_ms: 1038.333\n",
      "  load_throughput: 1608014.185\n",
      "  load_time_ms: 2.488\n",
      "  sample_throughput: 2905.281\n",
      "  sample_time_ms: 1376.803\n",
      "  update_time_ms: 4.217\n",
      "timestamp: 1606396464\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 76000\n",
      "training_iteration: 19\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-11-26_21-14-26\n",
      "done: false\n",
      "episode_len_mean: 176.06\n",
      "episode_reward_max: 200.0\n",
      "episode_reward_mean: 176.06\n",
      "episode_reward_min: 84.0\n",
      "episodes_this_iter: 23\n",
      "episodes_total: 1190\n",
      "experiment_id: 7493f658af8b447c956cf9373146e196\n",
      "hostname: gpu-53\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 3.814697322468419e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5517222881317139\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.00197978806681931\n",
      "      model: {}\n",
      "      policy_loss: 0.0013853969285264611\n",
      "      total_loss: 1951.3907470703125\n",
      "      vf_explained_var: 0.030600080266594887\n",
      "      vf_loss: 1951.3895263671875\n",
      "  num_steps_sampled: 80000\n",
      "  num_steps_trained: 80000\n",
      "iterations_since_restore: 20\n",
      "node_ip: 172.31.246.53\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 21.2\n",
      "  ram_util_percent: 14.0\n",
      "pid: 22008\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08396522123808482\n",
      "  mean_env_wait_ms: 0.08102916533264326\n",
      "  mean_inference_ms: 0.9824876465842809\n",
      "  mean_raw_obs_processing_ms: 0.10342950609842078\n",
      "time_since_restore: 49.79192280769348\n",
      "time_this_iter_s: 2.4017791748046875\n",
      "time_total_s: 49.79192280769348\n",
      "timers:\n",
      "  learn_throughput: 3860.82\n",
      "  learn_time_ms: 1036.049\n",
      "  load_throughput: 1545930.984\n",
      "  load_time_ms: 2.587\n",
      "  sample_throughput: 2913.957\n",
      "  sample_time_ms: 1372.704\n",
      "  update_time_ms: 4.219\n",
      "timestamp: 1606396466\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 80000\n",
      "training_iteration: 20\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-11-26_21-14-29\n",
      "done: false\n",
      "episode_len_mean: 176.96\n",
      "episode_reward_max: 200.0\n",
      "episode_reward_mean: 176.96\n",
      "episode_reward_min: 84.0\n",
      "episodes_this_iter: 22\n",
      "episodes_total: 1212\n",
      "experiment_id: 7493f658af8b447c956cf9373146e196\n",
      "hostname: gpu-53\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 1.9073486612342094e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5520260334014893\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.00022394802363123745\n",
      "      model: {}\n",
      "      policy_loss: -0.00579342944547534\n",
      "      total_loss: 1715.0894775390625\n",
      "      vf_explained_var: 0.049832794815301895\n",
      "      vf_loss: 1715.0953369140625\n",
      "  num_steps_sampled: 84000\n",
      "  num_steps_trained: 84000\n",
      "iterations_since_restore: 21\n",
      "node_ip: 172.31.246.53\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 19.4\n",
      "  ram_util_percent: 14.0\n",
      "pid: 22008\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0839649449620554\n",
      "  mean_env_wait_ms: 0.08102166026971747\n",
      "  mean_inference_ms: 0.9823832249853607\n",
      "  mean_raw_obs_processing_ms: 0.10317229824925932\n",
      "time_since_restore: 52.256693601608276\n",
      "time_this_iter_s: 2.464770793914795\n",
      "time_total_s: 52.256693601608276\n",
      "timers:\n",
      "  learn_throughput: 3863.111\n",
      "  learn_time_ms: 1035.435\n",
      "  load_throughput: 1546287.189\n",
      "  load_time_ms: 2.587\n",
      "  sample_throughput: 2915.032\n",
      "  sample_time_ms: 1372.198\n",
      "  update_time_ms: 4.213\n",
      "timestamp: 1606396469\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 84000\n",
      "training_iteration: 21\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-11-26_21-14-31\n",
      "done: false\n",
      "episode_len_mean: 178.1\n",
      "episode_reward_max: 200.0\n",
      "episode_reward_mean: 178.1\n",
      "episode_reward_min: 92.0\n",
      "episodes_this_iter: 23\n",
      "episodes_total: 1235\n",
      "experiment_id: 7493f658af8b447c956cf9373146e196\n",
      "hostname: gpu-53\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 9.536743306171047e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5489088892936707\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.0010513768065720797\n",
      "      model: {}\n",
      "      policy_loss: 0.008578197099268436\n",
      "      total_loss: 1892.005859375\n",
      "      vf_explained_var: 0.027790026739239693\n",
      "      vf_loss: 1891.997314453125\n",
      "  num_steps_sampled: 88000\n",
      "  num_steps_trained: 88000\n",
      "iterations_since_restore: 22\n",
      "node_ip: 172.31.246.53\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 21.433333333333334\n",
      "  ram_util_percent: 14.0\n",
      "pid: 22008\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08391637865743855\n",
      "  mean_env_wait_ms: 0.08098151257771724\n",
      "  mean_inference_ms: 0.9819085304723072\n",
      "  mean_raw_obs_processing_ms: 0.10287722404747436\n",
      "time_since_restore: 54.69945406913757\n",
      "time_this_iter_s: 2.442760467529297\n",
      "time_total_s: 54.69945406913757\n",
      "timers:\n",
      "  learn_throughput: 3868.643\n",
      "  learn_time_ms: 1033.954\n",
      "  load_throughput: 1456962.623\n",
      "  load_time_ms: 2.745\n",
      "  sample_throughput: 2922.39\n",
      "  sample_time_ms: 1368.743\n",
      "  update_time_ms: 4.042\n",
      "timestamp: 1606396471\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 88000\n",
      "training_iteration: 22\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-11-26_21-14-34\n",
      "done: false\n",
      "episode_len_mean: 180.87\n",
      "episode_reward_max: 200.0\n",
      "episode_reward_mean: 180.87\n",
      "episode_reward_min: 92.0\n",
      "episodes_this_iter: 21\n",
      "episodes_total: 1256\n",
      "experiment_id: 7493f658af8b447c956cf9373146e196\n",
      "hostname: gpu-53\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 4.7683716530855236e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5430896878242493\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.0002941675193142146\n",
      "      model: {}\n",
      "      policy_loss: 0.004468628671020269\n",
      "      total_loss: 1745.3817138671875\n",
      "      vf_explained_var: 0.02505033276975155\n",
      "      vf_loss: 1745.377197265625\n",
      "  num_steps_sampled: 92000\n",
      "  num_steps_trained: 92000\n",
      "iterations_since_restore: 23\n",
      "node_ip: 172.31.246.53\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 19.275\n",
      "  ram_util_percent: 14.0\n",
      "pid: 22008\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08387838045356336\n",
      "  mean_env_wait_ms: 0.08093342521726474\n",
      "  mean_inference_ms: 0.9817963090386024\n",
      "  mean_raw_obs_processing_ms: 0.10263427103579204\n",
      "time_since_restore: 57.12647104263306\n",
      "time_this_iter_s: 2.4270169734954834\n",
      "time_total_s: 57.12647104263306\n",
      "timers:\n",
      "  learn_throughput: 3851.485\n",
      "  learn_time_ms: 1038.56\n",
      "  load_throughput: 1493471.964\n",
      "  load_time_ms: 2.678\n",
      "  sample_throughput: 2941.282\n",
      "  sample_time_ms: 1359.951\n",
      "  update_time_ms: 4.073\n",
      "timestamp: 1606396474\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 92000\n",
      "training_iteration: 23\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-11-26_21-14-36\n",
      "done: false\n",
      "episode_len_mean: 182.19\n",
      "episode_reward_max: 200.0\n",
      "episode_reward_mean: 182.19\n",
      "episode_reward_min: 92.0\n",
      "episodes_this_iter: 21\n",
      "episodes_total: 1277\n",
      "experiment_id: 7493f658af8b447c956cf9373146e196\n",
      "hostname: gpu-53\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 2.3841858265427618e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5486845970153809\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.0003705624840222299\n",
      "      model: {}\n",
      "      policy_loss: -0.007195561192929745\n",
      "      total_loss: 1682.0830078125\n",
      "      vf_explained_var: 0.02614418976008892\n",
      "      vf_loss: 1682.0902099609375\n",
      "  num_steps_sampled: 96000\n",
      "  num_steps_trained: 96000\n",
      "iterations_since_restore: 24\n",
      "node_ip: 172.31.246.53\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 20.433333333333334\n",
      "  ram_util_percent: 14.0\n",
      "pid: 22008\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08383154429658117\n",
      "  mean_env_wait_ms: 0.08092109481422899\n",
      "  mean_inference_ms: 0.9817306926952725\n",
      "  mean_raw_obs_processing_ms: 0.10245920014256871\n",
      "time_since_restore: 59.5910701751709\n",
      "time_this_iter_s: 2.464599132537842\n",
      "time_total_s: 59.5910701751709\n",
      "timers:\n",
      "  learn_throughput: 3861.598\n",
      "  learn_time_ms: 1035.841\n",
      "  load_throughput: 1462372.61\n",
      "  load_time_ms: 2.735\n",
      "  sample_throughput: 2931.391\n",
      "  sample_time_ms: 1364.54\n",
      "  update_time_ms: 4.161\n",
      "timestamp: 1606396476\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 96000\n",
      "training_iteration: 24\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_metrics: {}\n",
      "date: 2020-11-26_21-14-39\n",
      "done: false\n",
      "episode_len_mean: 185.02\n",
      "episode_reward_max: 200.0\n",
      "episode_reward_mean: 185.02\n",
      "episode_reward_min: 111.0\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1297\n",
      "experiment_id: 7493f658af8b447c956cf9373146e196\n",
      "hostname: gpu-53\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 1.1920929132713809e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5397477149963379\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.0008436831994913518\n",
      "      model: {}\n",
      "      policy_loss: 0.0013212162302806973\n",
      "      total_loss: 2008.1478271484375\n",
      "      vf_explained_var: 0.05569212883710861\n",
      "      vf_loss: 2008.146240234375\n",
      "  num_steps_sampled: 100000\n",
      "  num_steps_trained: 100000\n",
      "iterations_since_restore: 25\n",
      "node_ip: 172.31.246.53\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 19.75\n",
      "  ram_util_percent: 14.0\n",
      "pid: 22008\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0837884350979748\n",
      "  mean_env_wait_ms: 0.08095518885010465\n",
      "  mean_inference_ms: 0.9826551200943996\n",
      "  mean_raw_obs_processing_ms: 0.1023470313408028\n",
      "time_since_restore: 62.0238995552063\n",
      "time_this_iter_s: 2.4328293800354004\n",
      "time_total_s: 62.0238995552063\n",
      "timers:\n",
      "  learn_throughput: 3866.686\n",
      "  learn_time_ms: 1034.478\n",
      "  load_throughput: 1465003.144\n",
      "  load_time_ms: 2.73\n",
      "  sample_throughput: 2925.051\n",
      "  sample_time_ms: 1367.498\n",
      "  update_time_ms: 4.212\n",
      "timestamp: 1606396479\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 100000\n",
      "training_iteration: 25\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-11-26_21-14-41\n",
      "done: false\n",
      "episode_len_mean: 187.5\n",
      "episode_reward_max: 200.0\n",
      "episode_reward_mean: 187.5\n",
      "episode_reward_min: 112.0\n",
      "episodes_this_iter: 22\n",
      "episodes_total: 1319\n",
      "experiment_id: 7493f658af8b447c956cf9373146e196\n",
      "hostname: gpu-53\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 5.9604645663569045e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5424789190292358\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.0016202179249376059\n",
      "      model: {}\n",
      "      policy_loss: 0.0017027604626491666\n",
      "      total_loss: 2112.766845703125\n",
      "      vf_explained_var: 0.05224085599184036\n",
      "      vf_loss: 2112.76513671875\n",
      "  num_steps_sampled: 104000\n",
      "  num_steps_trained: 104000\n",
      "iterations_since_restore: 26\n",
      "node_ip: 172.31.246.53\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 20.900000000000002\n",
      "  ram_util_percent: 14.0\n",
      "pid: 22008\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08379829317257016\n",
      "  mean_env_wait_ms: 0.08095442193080875\n",
      "  mean_inference_ms: 0.9825729388076262\n",
      "  mean_raw_obs_processing_ms: 0.10217990928891198\n",
      "time_since_restore: 64.54380869865417\n",
      "time_this_iter_s: 2.519909143447876\n",
      "time_total_s: 64.54380869865417\n",
      "timers:\n",
      "  learn_throughput: 3851.655\n",
      "  learn_time_ms: 1038.515\n",
      "  load_throughput: 1500323.365\n",
      "  load_time_ms: 2.666\n",
      "  sample_throughput: 2907.668\n",
      "  sample_time_ms: 1375.673\n",
      "  update_time_ms: 4.176\n",
      "timestamp: 1606396481\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 104000\n",
      "training_iteration: 26\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-11-26_21-14-44\n",
      "done: false\n",
      "episode_len_mean: 191.69\n",
      "episode_reward_max: 200.0\n",
      "episode_reward_mean: 191.69\n",
      "episode_reward_min: 112.0\n",
      "episodes_this_iter: 21\n",
      "episodes_total: 1340\n",
      "experiment_id: 7493f658af8b447c956cf9373146e196\n",
      "hostname: gpu-53\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 2.9802322831784522e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.545288622379303\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.00010161111276829615\n",
      "      model: {}\n",
      "      policy_loss: 0.002449938328936696\n",
      "      total_loss: 2291.927978515625\n",
      "      vf_explained_var: 0.0414295457303524\n",
      "      vf_loss: 2291.925537109375\n",
      "  num_steps_sampled: 108000\n",
      "  num_steps_trained: 108000\n",
      "iterations_since_restore: 27\n",
      "node_ip: 172.31.246.53\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 18.825\n",
      "  ram_util_percent: 14.0\n",
      "pid: 22008\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0837977806735974\n",
      "  mean_env_wait_ms: 0.08100458790557624\n",
      "  mean_inference_ms: 0.9835898178664257\n",
      "  mean_raw_obs_processing_ms: 0.10208610544400262\n",
      "time_since_restore: 67.05423927307129\n",
      "time_this_iter_s: 2.5104305744171143\n",
      "time_total_s: 67.05423927307129\n",
      "timers:\n",
      "  learn_throughput: 3852.97\n",
      "  learn_time_ms: 1038.16\n",
      "  load_throughput: 1551420.46\n",
      "  load_time_ms: 2.578\n",
      "  sample_throughput: 2898.601\n",
      "  sample_time_ms: 1379.976\n",
      "  update_time_ms: 3.941\n",
      "timestamp: 1606396484\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 108000\n",
      "training_iteration: 27\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-11-26_21-14-46\n",
      "done: false\n",
      "episode_len_mean: 190.88\n",
      "episode_reward_max: 200.0\n",
      "episode_reward_mean: 190.88\n",
      "episode_reward_min: 109.0\n",
      "episodes_this_iter: 21\n",
      "episodes_total: 1361\n",
      "experiment_id: 7493f658af8b447c956cf9373146e196\n",
      "hostname: gpu-53\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 1.4901161415892261e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.525536060333252\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.0002605689805932343\n",
      "      model: {}\n",
      "      policy_loss: -0.003408387303352356\n",
      "      total_loss: 1893.7091064453125\n",
      "      vf_explained_var: 0.025305552408099174\n",
      "      vf_loss: 1893.7125244140625\n",
      "  num_steps_sampled: 112000\n",
      "  num_steps_trained: 112000\n",
      "iterations_since_restore: 28\n",
      "node_ip: 172.31.246.53\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 20.075000000000003\n",
      "  ram_util_percent: 14.0\n",
      "pid: 22008\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0838307393053248\n",
      "  mean_env_wait_ms: 0.08103981684995168\n",
      "  mean_inference_ms: 0.9840317419137153\n",
      "  mean_raw_obs_processing_ms: 0.10195665298065042\n",
      "time_since_restore: 69.53697681427002\n",
      "time_this_iter_s: 2.4827375411987305\n",
      "time_total_s: 69.53697681427002\n",
      "timers:\n",
      "  learn_throughput: 3856.748\n",
      "  learn_time_ms: 1037.143\n",
      "  load_throughput: 1583129.606\n",
      "  load_time_ms: 2.527\n",
      "  sample_throughput: 2887.67\n",
      "  sample_time_ms: 1385.2\n",
      "  update_time_ms: 3.996\n",
      "timestamp: 1606396486\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 112000\n",
      "training_iteration: 28\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-11-26_21-14-49\n",
      "done: false\n",
      "episode_len_mean: 193.35\n",
      "episode_reward_max: 200.0\n",
      "episode_reward_mean: 193.35\n",
      "episode_reward_min: 109.0\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1381\n",
      "experiment_id: 7493f658af8b447c956cf9373146e196\n",
      "hostname: gpu-53\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 7.450580707946131e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5270232558250427\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.0003149059193674475\n",
      "      model: {}\n",
      "      policy_loss: -0.004617444239556789\n",
      "      total_loss: 1468.6851806640625\n",
      "      vf_explained_var: 0.0028539213817566633\n",
      "      vf_loss: 1468.6898193359375\n",
      "  num_steps_sampled: 116000\n",
      "  num_steps_trained: 116000\n",
      "iterations_since_restore: 29\n",
      "node_ip: 172.31.246.53\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 20.833333333333332\n",
      "  ram_util_percent: 14.0\n",
      "pid: 22008\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08386789853802248\n",
      "  mean_env_wait_ms: 0.08108493127297653\n",
      "  mean_inference_ms: 0.98429434537209\n",
      "  mean_raw_obs_processing_ms: 0.10185161541022084\n",
      "time_since_restore: 72.00890111923218\n",
      "time_this_iter_s: 2.471924304962158\n",
      "time_total_s: 72.00890111923218\n",
      "timers:\n",
      "  learn_throughput: 3839.428\n",
      "  learn_time_ms: 1041.822\n",
      "  load_throughput: 1581652.054\n",
      "  load_time_ms: 2.529\n",
      "  sample_throughput: 2891.538\n",
      "  sample_time_ms: 1383.347\n",
      "  update_time_ms: 3.944\n",
      "timestamp: 1606396489\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 116000\n",
      "training_iteration: 29\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-11-26_21-14-51\n",
      "done: false\n",
      "episode_len_mean: 193.17\n",
      "episode_reward_max: 200.0\n",
      "episode_reward_mean: 193.17\n",
      "episode_reward_min: 38.0\n",
      "episodes_this_iter: 21\n",
      "episodes_total: 1402\n",
      "experiment_id: 7493f658af8b447c956cf9373146e196\n",
      "hostname: gpu-53\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 3.7252903539730653e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5334668159484863\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.0002276003797305748\n",
      "      model: {}\n",
      "      policy_loss: -0.00894226785749197\n",
      "      total_loss: 1612.927490234375\n",
      "      vf_explained_var: 0.012679355219006538\n",
      "      vf_loss: 1612.9365234375\n",
      "  num_steps_sampled: 120000\n",
      "  num_steps_trained: 120000\n",
      "iterations_since_restore: 30\n",
      "node_ip: 172.31.246.53\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 19.35\n",
      "  ram_util_percent: 14.0\n",
      "pid: 22008\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08388009958282741\n",
      "  mean_env_wait_ms: 0.08113434363641106\n",
      "  mean_inference_ms: 0.9849629094128365\n",
      "  mean_raw_obs_processing_ms: 0.10174772470674799\n",
      "time_since_restore: 74.4426703453064\n",
      "time_this_iter_s: 2.4337692260742188\n",
      "time_total_s: 74.4426703453064\n",
      "timers:\n",
      "  learn_throughput: 3840.971\n",
      "  learn_time_ms: 1041.403\n",
      "  load_throughput: 1615772.868\n",
      "  load_time_ms: 2.476\n",
      "  sample_throughput: 2884.187\n",
      "  sample_time_ms: 1386.873\n",
      "  update_time_ms: 3.914\n",
      "timestamp: 1606396491\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 120000\n",
      "training_iteration: 30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(30):\n",
    "    result = agent.train()\n",
    "    print(pretty_print(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/ShenShuo/ray_results/PPO_CartPole-v0_2020-11-26_21-13-254pvkviun/checkpoint_30/checkpoint-30\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = agent.save()\n",
    "print(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-26 21:15:53,229\tWARNING worker.py:1091 -- WARNING: 9 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-11-26 21:15:53,241\tWARNING worker.py:1091 -- WARNING: 9 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-11-26 21:15:53,262\tWARNING worker.py:1091 -- WARNING: 10 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "/data/ShenShuo/miniconda3/envs/pets2/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/data/ShenShuo/miniconda3/envs/pets2/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "2020-11-26 21:16:03,779\tINFO trainable.py:255 -- Trainable.setup took 11.170 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2020-11-26 21:16:03,781\tWARNING util.py:40 -- Install gputil for GPU system monitoring.\n",
      "2020-11-26 21:16:03,988\tINFO trainable.py:482 -- Restored on 172.31.246.53 from checkpoint: /data/ShenShuo/ray_results/PPO_CartPole-v0_2020-11-26_21-13-254pvkviun/checkpoint_30/checkpoint-30\n",
      "2020-11-26 21:16:03,989\tINFO trainable.py:489 -- Current state after restoring: {'_iteration': 30, '_timesteps_total': None, '_time_total': 74.4426703453064, '_episodes_total': 1402}\n"
     ]
    }
   ],
   "source": [
    "trained_config = config.copy()\n",
    "\n",
    "test_agent = PPOTrainer(trained_config, 'CartPole-v0')\n",
    "test_agent.restore(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ShenShuo/miniconda3/envs/pets2/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "state = env.reset()\n",
    "done = False\n",
    "cumulative_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action = test_agent.compute_action(state)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    cumulative_reward += reward\n",
    "\n",
    "print(cumulative_reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.12 64-bit ('pets2': conda)",
   "language": "python",
   "name": "python361264bitpets2conda9be0b50ec2e74c71906ed808ba84e5ad"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
